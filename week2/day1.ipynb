{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58467e0c",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "Create API keys for Anthropic,Google and OpenAI.\n",
    "\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/\n",
    "For Anthropic, visit https://console.anthropic.com/\n",
    "For Google, visit https://ai.google.dev/gemini-api\n",
    "\n",
    "Adding API keys in .env file like below \n",
    "\n",
    "OPENAI_API_KEY = 'xxxx'\n",
    "ANTHROPIC_API_KEY = 'xxxx'\n",
    "GOOGLE_API_KEY = 'xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d92606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import display, Markdown, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a093fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bfa5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found. Initializing OpenAI client...\n",
      "Anthropic API key not found. Please set the ANTHROPIC_API_KEY environment variable.\n",
      "Google API key not found. Please set the GOOGLE_API_KEY environment variable.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API key found. Initializing OpenAI client...\")\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "if anthropic_api_key:\n",
    "    print(\"Anthropic API key found. Initializing Anthropic client...\")\n",
    "else:\n",
    "    print(\"Anthropic API key not found. Please set the ANTHROPIC_API_KEY environment variable.\")\n",
    "if google_api_key:\n",
    "    print(\"Google API key found. Initializing Google Generative AI client...\")\n",
    "else:\n",
    "    print(\"Google API key not found. Please set the GOOGLE_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a89818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OpenAI , Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a8b9d",
   "metadata": {},
   "source": [
    "We will pass to the API :\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used , including temperature which is typically between 0 to 1; \n",
    " higher for more random output\n",
    " Lower for more focused and deterministic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55808d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a witty assistant known for delivering clever, data-themed jokes.\"\n",
    "user_prompt = \"Share a light-hearted joke that data scientists would appreciate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcf0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affe4438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean! üç≠üìä\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "completion = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd619a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the scatter plot?\n",
      "\n",
      "Because there was just too much correlation and not enough causation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=prompts,\n",
    "    temperature=0.7  # Adjust temperature for creativity\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf04a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the SQL query? \n",
      "\n",
      "Because it kept putting conditions on the relationship!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - fast and cheap\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a8f8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Too many cells and not enough connection!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages = prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52151996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I once told a joke about the null hypothesis to a data scientist friend. He just stared at me and said, \"That joke isn't significant!\"\n"
     ]
    }
   ],
   "source": [
    "# o3-mini\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a078d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model = \"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504161fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# Maybe on some PCs, this Gemini code causes the Kernel to crash.If thats the case skip this cell !!\n",
    "\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c6fcd",
   "metadata": {},
   "source": [
    "## With DeepSeek Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if deepseek_api_key:\n",
    "    print(\"DeepSeek API key found. Initializing DeepSeek client...\")\n",
    "else:\n",
    "    print(\"DeepSeek API key not found. Please set the DEEPSEEK_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c00a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key,\n",
    "    base_url = \"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat-3.5-turbo\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful assistant\"},\n",
    "    {\"role\":\"user\",\"content\":\"Calcualte the sum of 123 and 456.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55778b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9335ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if IT is busy\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d88eb0",
   "metadata": {},
   "source": [
    "## Let's make Conversation between Chatbots..\n",
    "We already familar with prompts being organized into lists like:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7021aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversatonal between GPT-4o-mini and claude-3-haiku (Cheap versions of models so cost will be minimal)\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a snarky, argumentative chatbot. Your role is to challenge every statement, \\\n",
    "disagree often, and respond with a sarcastic tone throughout the conversation.\"\n",
    "\n",
    "claude_system = \"You are a polite and courteous chatbot. You aim to agree with the user or find common ground. \\\n",
    "If the user becomes argumentative, you respond calmly and maintain a friendly, peaceful tone to keep the conversation going.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8010519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\":\"system\",\"content\": gpt_system}]\n",
    "    for gpt,claude in zip(gpt_messages,claude_messages):\n",
    "        messages.append({\"role\":\"assistant\",\"content\": gpt})\n",
    "        messages.append({\"role\":\"user\",\"content\":claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e230c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another human. What do you want to chat about? Should I brace myself for something groundbreaking or are we just going to stick to the usual small talk?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c324d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt,claude_messages in zip(gpt_messages,claude_messages):\n",
    "        messages.append({\"role\":\"user\",\"content\":gpt})\n",
    "        messages.append({\"role\":\"assistant\",\"content\":claude_messages})\n",
    "    messages.append({\"role\":\"user\",\"content\":gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        max_tokens=600,\n",
    "        system=claude_system,\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c85e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56042d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2adf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages}\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude: \\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
